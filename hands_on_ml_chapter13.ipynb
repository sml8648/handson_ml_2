{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hands-on-ml chapter13.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPMwuNzb5Vjwvy7/I5WWCC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sml8648/handson_ml_2/blob/main/hands_on_ml_chapter13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lIH3Cg2VmAo"
      },
      "outputs": [],
      "source": [
        "# 파이썬 ≥3.5 필수\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# 사이킷런 ≥0.20 필수\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version은 코랩 명령입니다.\n",
        "    %tensorflow_version 2.x\n",
        "    %pip install -q -U tfx\n",
        "    print(\"패키지 호환 에러는 무시해도 괜찮습니다.\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 텐서플로 ≥2.0 필수\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# 공통 모듈 임포트\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
        "np.random.seed(42)\n",
        "\n",
        "# 깔끔한 그래프 출력을 위해\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# 그림을 저장할 위치\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"data\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"그림 저장:\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "RgbneI5r270M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "metadata": {
        "id": "BNWQR3uU28hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.range(10)"
      ],
      "metadata": {
        "id": "1X54Pddq3M-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "W90ZusKJ3ZTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "d69l-HiH3dir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda x:x*2)"
      ],
      "metadata": {
        "id": "EpaTkIQ43lBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "E8raECmC37jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.unbatch()"
      ],
      "metadata": {
        "id": "ok7Salfv398p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda x : x < 10)"
      ],
      "metadata": {
        "id": "FyzXphHp4BQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "YCi92Ge94NU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "nZzQ4Qpe4RcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 캘리포니아 주택 데이터셋을 여러개의 csv로 나누기"
      ],
      "metadata": {
        "id": "OPakzUnZ4jMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1,1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_"
      ],
      "metadata": {
        "id": "a3SwyQVI4lag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datsets\",\"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir,\"my_{}_{:02d}.csv\")\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m),n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    \n",
        "    return filepaths"
      ],
      "metadata": {
        "id": "fFE93gCa5KdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + ['MedianHouseValue']\n",
        "header = ','.join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, 'train', header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, 'train', header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, 'train', header, n_parts=10)"
      ],
      "metadata": {
        "id": "bfTZF1d36G1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(train_filepaths[0]).head()"
      ],
      "metadata": {
        "id": "ixzYhyam7HDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_filepaths[0]) as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline(), end='')"
      ],
      "metadata": {
        "id": "96QWh_Fj7NGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 입력 파이프라인"
      ],
      "metadata": {
        "id": "7x77nxOC7V_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "metadata": {
        "id": "eW3r7L4-7W-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filepath in filepath_dataset:\n",
        "    print(filepath)"
      ],
      "metadata": {
        "id": "xi3J3cjT7epu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)"
      ],
      "metadata": {
        "id": "eDzNyyqa72Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in dataset.take(5):\n",
        "    print(line.numpy())"
      ],
      "metadata": {
        "id": "urfRIxnH8P9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\n",
        "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
        "parsed_fields"
      ],
      "metadata": {
        "id": "xpWu5lpv8Qhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_fields = tf.io.decode_csv(',,,,5',record_defaults)\n",
        "parsed_fields"
      ],
      "metadata": {
        "id": "8gmTGuJ398tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    parsed_fields = tf.io.decode_csv(',,,,',record_defaults)\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "    print(ex)"
      ],
      "metadata": {
        "id": "Su0VFc91--Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = 8\n",
        "\n",
        "@tf.function\n",
        "def preprocess(line):\n",
        "    defs = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean) / X_std, y"
      ],
      "metadata": {
        "id": "PO1JPqOO_Jay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
      ],
      "metadata": {
        "id": "mOWZqAqAAU5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length = n_readers, num_parallel_calls=n_read_threads)\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "dEFqw8cOAoqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
        "for X_batch, y_batch in train_set.take(2):\n",
        "    print(\"X = \", X_batch)\n",
        "    print(\"Y = \", y_batch)\n",
        "    print()"
      ],
      "metadata": {
        "id": "K6G5NdZVBcDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "metadata": {
        "id": "OZDMg36aB3s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "-klNt_zZCD75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
      ],
      "metadata": {
        "id": "BqSokynnCWPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set)"
      ],
      "metadata": {
        "id": "b5e6Z7v7CeyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_set, steps=len(X_test) // batch_size)"
      ],
      "metadata": {
        "id": "J-MPEYR_CpfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_set = test_set.map(lambda X, y : X)\n",
        "X_new = X_test\n",
        "model.predict(new_set, steps=len(X_new) // batch_size)"
      ],
      "metadata": {
        "id": "VNjO1knxCzWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps_per_epoch = len(X_train) // batch_size\n",
        "total_steps = n_epochs*n_steps_per_epoch\n",
        "global_step = 0\n",
        "\n",
        "for X_batch, y_batch in train_set.take(total_steps):\n",
        "    global_step += 1\n",
        "    print('\\rGlobal step {}/{}'.format(global_step, total_steps), end=\"\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_batch)\n",
        "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "    \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "sS4HUXOyC5OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "@tf.function\n",
        "def train(model, n_epochs, batch_size=32,\n",
        "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
        "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
        "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
        "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
        "    n_steps_per_epoch = len(X_train) // batch_size\n",
        "    total_steps = n_epochs * n_steps_per_epoch\n",
        "    global_step = 0\n",
        "    for X_batch, y_batch in train_set.take(total_steps):\n",
        "        global_step += 1\n",
        "        if tf.equal(global_step % 100, 0):\n",
        "            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "train(model, 6)"
      ],
      "metadata": {
        "id": "YQCVjhLpFg7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFRecord 이진 포맷"
      ],
      "metadata": {
        "id": "vkIrCOwNIe5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
        "    f.write(b'123')\n",
        "    f.write(b'xyz314')"
      ],
      "metadata": {
        "id": "6GJ6_y8wIhQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = ['my_data.tfrecord']\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "_9hL4wDlJehS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = ['my_test_{}.tfrecord'.format(i) for i in range(5)]\n",
        "\n",
        "for i, filepath in enumerate(filepaths):\n",
        "    with tf.io.TFRecordWriter(filepath) as f:\n",
        "        for j in range(3):\n",
        "            f.write(\"File {} record {}\".format(i,j).encode('utf-8'))\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "NJQria_RrCTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
        "with tf.io.TFRecordWriter('my_compressed.tfrecord',options) as f:\n",
        "    f.write(b'This is the first record')\n",
        "    f.write(b'And this is the second record')"
      ],
      "metadata": {
        "id": "ZxfFAMxTrMmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'],\n",
        "                                  compression_type='GZIP')\n",
        "\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "vTzuQt-8ryh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 프로토콜 버퍼"
      ],
      "metadata": {
        "id": "2jNJlNIGsEV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile person.proto\n",
        "syntax = 'proto3';\n",
        "message Person {\n",
        "    string name = 1;\n",
        "    int32 id = 2;\n",
        "    repeated string email = 3;\n",
        "}"
      ],
      "metadata": {
        "id": "bSfhG5MgsGRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
      ],
      "metadata": {
        "id": "Hxs82USQsPY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls person*"
      ],
      "metadata": {
        "id": "Fy0V2muSsbrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from person_pb2 import Person\n",
        "\n",
        "person = Person(name='Al', id=123, email=['a@b.com'])\n",
        "print(person)"
      ],
      "metadata": {
        "id": "_uMVn_j5sdu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.name"
      ],
      "metadata": {
        "id": "OpPcxwspsoet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.name = 'Alice'"
      ],
      "metadata": {
        "id": "vx1O8aPrsp97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.email[0]"
      ],
      "metadata": {
        "id": "c6XHUrBBsrsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.email.append(\"c@d.com\")"
      ],
      "metadata": {
        "id": "Gzww496XsuIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = person.SerializeToString()\n",
        "s"
      ],
      "metadata": {
        "id": "nR4YrlX8swH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person2 = Person()\n",
        "person2.ParseFromString(s)"
      ],
      "metadata": {
        "id": "QRveLWAzs0NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person == person2"
      ],
      "metadata": {
        "id": "UmyT8nXYs6tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 사용자 정의 protobuf"
      ],
      "metadata": {
        "id": "REFxKZIqtADT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "person_tf = tf.io.decode_proto(\n",
        "    bytes=s,\n",
        "    message_type='Person',\n",
        "    field_names=['name','id','email'],\n",
        "    output_types=[tf.string, tf.int32, tf.string],\n",
        "    descriptor_source='person.desc'\n",
        ")\n",
        "\n",
        "person_tf.values"
      ],
      "metadata": {
        "id": "pIhoh72RtCbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
        "        }))\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString())"
      ],
      "metadata": {
        "id": "VwfNAv4DtSVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_description = {\n",
        "    'name':tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    'id':tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    'emails':tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "for serialized_example in tf.data.TFRecordDataset(['my_contacts.tfrecord']):\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)"
      ],
      "metadata": {
        "id": "w394n-UwuIZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_example"
      ],
      "metadata": {
        "id": "7y9a5tyKu4_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_example"
      ],
      "metadata": {
        "id": "rkz4dyn9vD-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_example['emails'].values[0]"
      ],
      "metadata": {
        "id": "Fd72GnBBvFIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.sparse.to_dense(parsed_example['emails'], default_value=b\"\")"
      ],
      "metadata": {
        "id": "v8fuxMUxvJmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_example['emails'].values"
      ],
      "metadata": {
        "id": "7_-lPflvvQpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> TFRecord에 이미지 넣기"
      ],
      "metadata": {
        "id": "p7iYyNeIvVGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_sample_images\n",
        "\n",
        "img = load_sample_images()['images'][0]\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title('Original Image')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qm3GV9GovXTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.io.encode_jpeg(img)\n",
        "example_with_image = Example(features=Features(feature={\n",
        "    \"image\": Feature(bytes_list=BytesList(value=[data.numpy()]))}))\n",
        "serialized_example = example_with_image.SerializeToString()"
      ],
      "metadata": {
        "id": "g70pam7hvnq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_description = { 'image':tf.io.VarLenFeature(tf.string)}\n",
        "example_with_image = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "decoded_img = tf.io.decode_jpeg(example_with_image['image'].values[0])"
      ],
      "metadata": {
        "id": "HPNYVZhDwDG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_img = tf.io.decode_image(example_with_image['image'].values[0])"
      ],
      "metadata": {
        "id": "1q0xqtXNwfxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(decoded_img)\n",
        "plt.title('Decoded Image')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "me7kL-htwnpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> TFRecord에 텐서와 희소 텐서 넣기"
      ],
      "metadata": {
        "id": "MKOtfSARwvy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = tf.constant([[0.,1.],[2.,3.],[4.,5.]])\n",
        "s = tf.io.serialize_tensor(t)\n",
        "s"
      ],
      "metadata": {
        "id": "7I3_ARycwyZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.io.parse_tensor(s, out_type=tf.float32)"
      ],
      "metadata": {
        "id": "C74T4ljmw9xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serialized_sparse = tf.io.serialize_sparse(parsed_example['emails'])\n",
        "serialized_sparse"
      ],
      "metadata": {
        "id": "0erocyr2xB4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BytesList(value=serialized_sparse.numpy())"
      ],
      "metadata": {
        "id": "JsNT0Q6sxjzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset(['my_contacts.tfrecord']).batch(10)\n",
        "for serialized_examples in dataset:\n",
        "    parsed_examples = tf.io.parse_example(serialized_examples,\n",
        "                                          feature_description)"
      ],
      "metadata": {
        "id": "VADHO4Dexouo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_examples"
      ],
      "metadata": {
        "id": "3HgGKqIQx39k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SequenceExample를 사용해 순차 데이터 다루기"
      ],
      "metadata": {
        "id": "p7W_0FlUx7Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n",
        "\n",
        "context = Features(feature={\n",
        "    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n",
        "    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n",
        "    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n",
        "})\n",
        "\n",
        "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n",
        "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
        "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
        "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
        "\n",
        "def words_to_feature(words):\n",
        "    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n",
        "                                               for word in words]))\n",
        "\n",
        "content_features = [words_to_feature(sentence) for sentence in content]\n",
        "comments_features = [words_to_feature(comment) for comment in comments]\n",
        "            \n",
        "sequence_example = SequenceExample(\n",
        "    context=context,\n",
        "    feature_lists=FeatureLists(feature_list={\n",
        "        \"content\": FeatureList(feature=content_features),\n",
        "        \"comments\": FeatureList(feature=comments_features)\n",
        "    }))"
      ],
      "metadata": {
        "id": "L70K_iwNyBcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_example"
      ],
      "metadata": {
        "id": "zvrW5rpMyjOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serialized_sequence_example = sequence_example.SerializeToString()"
      ],
      "metadata": {
        "id": "27PcQCooyliy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_feature_descriptions = {\n",
        "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"title\": tf.io.VarLenFeature(tf.string),\n",
        "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n",
        "}\n",
        "sequence_feature_descriptions = {\n",
        "    \"content\": tf.io.VarLenFeature(tf.string),\n",
        "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
        "    serialized_sequence_example, context_feature_descriptions,\n",
        "    sequence_feature_descriptions)"
      ],
      "metadata": {
        "id": "ohufeAFEytGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_context"
      ],
      "metadata": {
        "id": "Idw8DfDk0-lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_context['title'].values"
      ],
      "metadata": {
        "id": "1CxyMhNQ1JBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_feature_lists"
      ],
      "metadata": {
        "id": "46x_zfG_1Lmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.RaggedTensor.from_sparse(parsed_feature_lists['content']))"
      ],
      "metadata": {
        "id": "HxoNSmBL1Oaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 특성 API"
      ],
      "metadata": {
        "id": "xr5Jm3id3LQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    os.makedirs(housing_path, exist_ok=True)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()"
      ],
      "metadata": {
        "id": "pOb-6xjJ3NAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_housing_data()"
      ],
      "metadata": {
        "id": "kh7rFyeV3WGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
        "    return pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "RvCl6juS3Xyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing = load_housing_data()\n",
        "housing.head()"
      ],
      "metadata": {
        "id": "7jystPXc3iv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_median_age = tf.feature_column.numeric_column('housing_median_age')"
      ],
      "metadata": {
        "id": "3j0JyhMP3l-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_mean, age_std = X_mean[1], X_std[1]\n",
        "housing_median_age = tf.feature_column.numeric_column(\n",
        "    'housing_median_age',normalizer_fn=lambda x:(x - age_mean) / age_std)"
      ],
      "metadata": {
        "id": "h21epcu13tzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_income = tf.feature_column.numeric_column('median_income')\n",
        "bucketized_income = tf.feature_column.bucketized_column(median_income, boundaries=[1.5,3.,4.5,6.])"
      ],
      "metadata": {
        "id": "jrPRuNOG4Dzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucketized_income"
      ],
      "metadata": {
        "id": "ueVXfqTB4m7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
        "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    \"ocean_proximity\", ocean_prox_vocab)"
      ],
      "metadata": {
        "id": "Y4ntzP6c4pzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocean_proximity"
      ],
      "metadata": {
        "id": "WGKd3xIR4vip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_hash = tf.feature_column.categorical_column_with_hash_bucket('city',hash_bucket_size=1000)\n",
        "city_hash"
      ],
      "metadata": {
        "id": "XyVPVK6P4xWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucketized_age = tf.feature_column.bucketized_column(\n",
        "    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\n",
        "age_and_ocean_proximity = tf.feature_column.crossed_column(\n",
        "    [bucketized_age, ocean_proximity], hash_bucket_size=100)"
      ],
      "metadata": {
        "id": "LqTJUp2Q4_4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
        "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
        "bucketized_latitude = tf.feature_column.bucketized_column(\n",
        "    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\n",
        "bucketized_longitude = tf.feature_column.bucketized_column(\n",
        "    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\n",
        "location = tf.feature_column.crossed_column(\n",
        "    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)"
      ],
      "metadata": {
        "id": "C0Iz576Wje9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)\n",
        "\n",
        "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
        "                                                           dimension=2)"
      ],
      "metadata": {
        "id": "otOFOkW0jjuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature_column을 사용해 파싱하기"
      ],
      "metadata": {
        "id": "AFwcEPU9jl5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "median_house_value = tf.feature_column.numeric_column(\"median_house_value\")"
      ],
      "metadata": {
        "id": "fGfyQJILjpfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [housing_median_age, median_house_value]\n",
        "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)\n",
        "feature_descriptions"
      ],
      "metadata": {
        "id": "6TXrBCLAjqbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.io.TFRecordWriter(\"my_data_with_features.tfrecords\") as f:\n",
        "    for x, y in zip(X_train[:, 1:2], y_train):\n",
        "        example = Example(features=Features(feature={\n",
        "            \"housing_median_age\": Feature(float_list=FloatList(value=[x])),\n",
        "            \"median_house_value\": Feature(float_list=FloatList(value=[y]))\n",
        "        }))\n",
        "        f.write(example.SerializeToString())"
      ],
      "metadata": {
        "id": "oEzbVN3xjrVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "-Cfha27Pjsaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_examples(serialized_examples):\n",
        "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
        "    targets = examples.pop(\"median_house_value\") # separate the targets\n",
        "    return examples, targets\n",
        "\n",
        "batch_size = 32\n",
        "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
        "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)"
      ],
      "metadata": {
        "id": "yQ8XfWKHjtiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_without_target = columns[:-1]\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(dataset, steps_per_epoch=len(X_train) // batch_size, epochs=5)"
      ],
      "metadata": {
        "id": "gbyT8wYhjum4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_columns = [ocean_proximity_embed, bucketized_income]\n",
        "dense_features = keras.layers.DenseFeatures(some_columns)\n",
        "dense_features({\n",
        "    \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\n",
        "    \"median_income\": [[3.], [7.2], [1.]]\n",
        "})"
      ],
      "metadata": {
        "id": "Qx7nPln-jvdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF 변환"
      ],
      "metadata": {
        "id": "oxa_SZvBjxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import tensorflow_transform as tft\n",
        "\n",
        "    def preprocess(inputs):  # inputs is a batch of input features\n",
        "        median_age = inputs[\"housing_median_age\"]\n",
        "        ocean_proximity = inputs[\"ocean_proximity\"]\n",
        "        standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
        "        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
        "        return {\n",
        "            \"standardized_median_age\": standardized_age,\n",
        "            \"ocean_proximity_id\": ocean_proximity_id\n",
        "        }\n",
        "except ImportError:\n",
        "    print(\"TF Transform is not installed. Try running: pip3 install -U tensorflow-transform\")"
      ],
      "metadata": {
        "id": "1uJjvD4wjyG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텐서플로 데이터셋"
      ],
      "metadata": {
        "id": "dIqCcplHjzqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets = tfds.load(name=\"mnist\")\n",
        "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
      ],
      "metadata": {
        "id": "zOmYA0p7j04q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfds.list_builders())"
      ],
      "metadata": {
        "id": "qc1bbRy3j21K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
        "for item in mnist_train:\n",
        "    images = item[\"image\"]\n",
        "    labels = item[\"label\"]\n",
        "    for index in range(5):\n",
        "        plt.subplot(1, 5, index + 1)\n",
        "        image = images[index, ..., 0]\n",
        "        label = labels[index].numpy()\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.title(label)\n",
        "        plt.axis(\"off\")\n",
        "    break"
      ],
      "metadata": {
        "id": "rY9xl3npj3sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = tfds.load(name=\"mnist\")\n",
        "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
        "mnist_train = mnist_train.repeat(5).batch(32)\n",
        "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
        "mnist_train = mnist_train.prefetch(1)\n",
        "for images, labels in mnist_train.take(1):\n",
        "    print(images.shape)\n",
        "    print(labels.numpy())"
      ],
      "metadata": {
        "id": "ccKjln4Lj5bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "kibhd978kEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
        "mnist_train = datasets[\"train\"].repeat().prefetch(1)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)"
      ],
      "metadata": {
        "id": "enAuBCSqkFGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텐서플로 허브"
      ],
      "metadata": {
        "id": "wwFiuLK6kKfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "nfa6bP4gkLVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\",\n",
        "                           output_shape=[50], input_shape=[], dtype=tf.string)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(keras.layers.Dense(16, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "DSZCgTnSkMNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = tf.constant([\"It was a great movie\", \"The actors were amazing\"])\n",
        "embeddings = hub_layer(sentences)"
      ],
      "metadata": {
        "id": "8e-rQTNTkNI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "q6IxDql7kOKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 연습문제"
      ],
      "metadata": {
        "id": "X4l41f2IkP-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.\n",
        "<h3> a."
      ],
      "metadata": {
        "id": "tANMBTExkSnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "metadata": {
        "id": "wX0jsu7mkVWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "hcwbrX-nkWah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
        "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
        "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "pG2O21uIkXmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_example(image, label):\n",
        "    image_data = tf.io.serialize_tensor(image)\n",
        "    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n",
        "    return Example(\n",
        "        features=Features(\n",
        "            feature={\n",
        "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
        "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
        "            }))"
      ],
      "metadata": {
        "id": "NioAy0GwkYjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in valid_set.take(1):\n",
        "    print(create_example(image, label))"
      ],
      "metadata": {
        "id": "Xax6Uz0bkZh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import ExitStack\n",
        "\n",
        "def write_tfrecords(name, dataset, n_shards=10):\n",
        "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
        "             for index in range(n_shards)]\n",
        "    with ExitStack() as stack:\n",
        "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
        "                   for path in paths]\n",
        "        for index, (image, label) in dataset.enumerate():\n",
        "            shard = index % n_shards\n",
        "            example = create_example(image, label)\n",
        "            writers[shard].write(example.SerializeToString())\n",
        "    return paths"
      ],
      "metadata": {
        "id": "tGmojvkrkaYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
        "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
        "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
      ],
      "metadata": {
        "id": "K04vHccCkdsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> b."
      ],
      "metadata": {
        "id": "5tqltDCekfNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(tfrecord):\n",
        "    feature_descriptions = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
        "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
        "    #image = tf.io.decode_jpeg(example[\"image\"])\n",
        "    image = tf.reshape(image, shape=[28, 28])\n",
        "    return image, example[\"label\"]\n",
        "\n",
        "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
        "                  n_parse_threads=5, batch_size=32, cache=True):\n",
        "    dataset = tf.data.TFRecordDataset(filepaths,\n",
        "                                      num_parallel_reads=n_read_threads)\n",
        "    if cache:\n",
        "        dataset = dataset.cache()\n",
        "    if shuffle_buffer_size:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "ivX-R1_Zkgv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
        "valid_set = mnist_dataset(valid_filepaths)\n",
        "test_set = mnist_dataset(test_filepaths)"
      ],
      "metadata": {
        "id": "PAB4ql-hkh4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train_set.take(1):\n",
        "    for i in range(5):\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(str(y[i].numpy()))"
      ],
      "metadata": {
        "id": "AxdoJL_8ki4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class Standardization(keras.layers.Layer):\n",
        "    def adapt(self, data_sample):\n",
        "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
        "    def call(self, inputs):\n",
        "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
        "\n",
        "standardization = Standardization(input_shape=[28, 28])\n",
        "# or perhaps soon:\n",
        "#standardization = keras.layers.Normalization()\n",
        "\n",
        "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
        "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
        "                               axis=0).astype(np.float32)\n",
        "standardization.adapt(sample_images)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    standardization,\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "sgsI17ZPkjro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "logs = os.path.join(os.curdir, \"my_logs\",\n",
        "                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=logs, histogram_freq=1, profile_batch=10)\n",
        "\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set,\n",
        "          callbacks=[tensorboard_cb])"
      ],
      "metadata": {
        "id": "BIFWcNttkk4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs --port=6006"
      ],
      "metadata": {
        "id": "UWYzBaQ-kmNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.\n",
        "<h3> a."
      ],
      "metadata": {
        "id": "iyqPooRpkqd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "path"
      ],
      "metadata": {
        "id": "31LLD4fJkr6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, subdirs, files in os.walk(path):\n",
        "    indent = len(Path(name).parts) - len(path.parts)\n",
        "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
        "    for index, filename in enumerate(sorted(files)):\n",
        "        if index == 3:\n",
        "            print(\"    \" * (indent + 1) + \"...\")\n",
        "            break\n",
        "        print(\"    \" * (indent + 1) + filename)"
      ],
      "metadata": {
        "id": "6CzzONPhktEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_paths(dirpath):\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
        "\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ],
      "metadata": {
        "id": "L7497YLakuCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> b."
      ],
      "metadata": {
        "id": "X4lsgLnPkvsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(test_valid_pos)\n",
        "\n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ],
      "metadata": {
        "id": "u9EeVVzCkwof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> c."
      ],
      "metadata": {
        "id": "krFUF2ijkydE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
        "        for filepath in filepaths:\n",
        "            with open(filepath) as review_file:\n",
        "                reviews.append(review_file.read())\n",
        "            labels.append(label)\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.constant(reviews), tf.constant(labels)))"
      ],
      "metadata": {
        "id": "kIHproyxkzfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
        "    print(X)\n",
        "    print(y)\n",
        "    print()"
      ],
      "metadata": {
        "id": "3MSf0iCyk0XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
      ],
      "metadata": {
        "id": "rkwxJrZUk1LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
        "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
        "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
        "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)\n"
      ],
      "metadata": {
        "id": "1gkLUfsHk2Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
      ],
      "metadata": {
        "id": "jTB7rOq5k284"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
      ],
      "metadata": {
        "id": "yVHsaAPRk3vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
        "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
        "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "6kdmHssulBNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> d."
      ],
      "metadata": {
        "id": "WqQGXtTOlC0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch, n_words=50):\n",
        "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
        "    Z = tf.strings.substr(X_batch, 0, 300)\n",
        "    Z = tf.strings.lower(Z)\n",
        "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
        "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
        "    Z = tf.strings.split(Z)\n",
        "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
        "\n",
        "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
        "preprocess(X_example)"
      ],
      "metadata": {
        "id": "x932wBdklDb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_vocabulary(data_sample, max_size=1000):\n",
        "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
        "    counter = Counter()\n",
        "    for words in preprocessed_reviews:\n",
        "        for word in words:\n",
        "            if word != b\"<pad>\":\n",
        "                counter[word] += 1\n",
        "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
        "\n",
        "get_vocabulary(X_example)"
      ],
      "metadata": {
        "id": "dIHqXKe0lEh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextVectorization(keras.layers.Layer):\n",
        "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.max_vocabulary_size = max_vocabulary_size\n",
        "        self.n_oov_buckets = n_oov_buckets\n",
        "\n",
        "    def adapt(self, data_sample):\n",
        "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
        "        words = tf.constant(self.vocab)\n",
        "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
        "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        preprocessed_inputs = preprocess(inputs)\n",
        "        return self.table.lookup(preprocessed_inputs)"
      ],
      "metadata": {
        "id": "7bA8EjqelFvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization()\n",
        "\n",
        "text_vectorization.adapt(X_example)\n",
        "text_vectorization(X_example)"
      ],
      "metadata": {
        "id": "VpC3GbcalJov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocabulary_size = 1000\n",
        "n_oov_buckets = 100\n",
        "\n",
        "sample_review_batches = train_set.map(lambda review, label: review)\n",
        "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
        "                                axis=0)\n",
        "\n",
        "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
        "                                       input_shape=[])\n",
        "text_vectorization.adapt(sample_reviews)"
      ],
      "metadata": {
        "id": "2Lt02dOMlK8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(X_example)"
      ],
      "metadata": {
        "id": "yhDF2Mb_lL7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.vocab[:10]"
      ],
      "metadata": {
        "id": "hvLvc6KNlMwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
        "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
      ],
      "metadata": {
        "id": "oFoWhckzlZ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BagOfWords(keras.layers.Layer):\n",
        "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.n_tokens = n_tokens\n",
        "    def call(self, inputs):\n",
        "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
        "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
      ],
      "metadata": {
        "id": "0nbGX06LlbMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = BagOfWords(n_tokens=4)\n",
        "bag_of_words(simple_example)"
      ],
      "metadata": {
        "id": "5RTi1QrclcFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\n",
        "bag_of_words = BagOfWords(n_tokens)"
      ],
      "metadata": {
        "id": "B39CylfZlc04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    text_vectorization,\n",
        "    bag_of_words,\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "metadata": {
        "id": "qOw6mrTVldlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> e."
      ],
      "metadata": {
        "id": "SNIkW5BylfN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mean_embedding(inputs):\n",
        "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
        "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
        "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
        "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
        "\n",
        "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
        "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
        "compute_mean_embedding(another_example)"
      ],
      "metadata": {
        "id": "fSTcTkmRlgNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)"
      ],
      "metadata": {
        "id": "xZGN8wPxlhR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)"
      ],
      "metadata": {
        "id": "MCQQpxhjlh95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 20\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    text_vectorization,\n",
        "    keras.layers.Embedding(input_dim=n_tokens,\n",
        "                           output_dim=embedding_size,\n",
        "                           mask_zero=True), # <pad> tokens => zero vectors\n",
        "    keras.layers.Lambda(compute_mean_embedding),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "yDIfQawAlkTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> f."
      ],
      "metadata": {
        "id": "ziJi4B-mll6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "metadata": {
        "id": "JnDf6oUilmqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> g."
      ],
      "metadata": {
        "id": "1EAeqxmhloCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets = tfds.load(name=\"imdb_reviews\")\n",
        "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
      ],
      "metadata": {
        "id": "awdnuSDIlov4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in train_set.take(1):\n",
        "    print(example[\"text\"])\n",
        "    print(example[\"label\"])"
      ],
      "metadata": {
        "id": "0vi8Maz6lpwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vo8ff71clqcA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}